### Exercise 2.2-1

Express the function $n^3/1000 - 100n^2 - 100n + 3$ in terms of $\Theta$-notation

$$
\Theta(n^3)
$$

### Exercise 2.2-2

Consider sorting n numbers stored in array A by first finding the smallest element
of A and exchanging it with the element in A[1]. Then find the second smallest
element of A, and exchange it with A[2]. Continue in this manner for the first n-1
elements of A. Write pseudocode for this algorithm, which is known as **_selection
sort_**. What loop invariant does this algorithm maintain? Why does it need to run
for only the first n-1 1 elements, rather than for all n elements? Give the best-case
and worst-case running times of selection sort in $\Theta$-notation.

    SELECTION-SORT(A)
        for i = 1 to A.length - 1
            t = i
            for j = i + 1 to A.length
                if A[j] < A[t]
                    t = j
            swap A[i] and A[t]
**loop invariant**:  
At the start of each iteration of **for** loop of lines 1-6, the subarry A[1..i-1] contains the i-1 smallest elements of A, and in sorted order.  
The best-case and worst-case is both $\Theta(n^2)$  

### Exercise 2.2-3

Consider linear search again (see Exercise 2.1-3). How many elements of the input sequence need to be checked on the average, assuming that the element being
searched for is equally likely to be any element in the array? How about in the
worst case? What are the average-case and worst-case running times of linear
search in $\Theta$-notation? Justify your answers.

The best-case is that the first element is the target that we want, the worst-case is that the target is the last element of A or it doesn't exist in A. So the average-case is $\Theta(\frac{n}{2})$ and the worst-case is $\Theta(n)$.

### Exercise 2.2-4

How can we modify almost any algorithm to have a good best-case running time?

We can check if input is already the result we want before the algorithm begin, return if it does, so it's the best-case. But it will increase the running time when it's a worst-case. :(